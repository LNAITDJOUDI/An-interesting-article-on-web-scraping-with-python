{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Objectif:\n",
    "\n",
    "  Récupération des données sur le web avec le scraping \n",
    " \n",
    "\n",
    "## Source:\n",
    "   \n",
    "   Récupération des données de France sur le site https://www.journaldunet.com/\n",
    "\n",
    "## Methodes:\n",
    "\n",
    "  Utilisation des librairies Python pour le scraping web (bs4,selenium,requestes ....)\n",
    " \n",
    " ## Resultas:\n",
    "\n",
    "  Fichiers csv des ville de France leur démographie, Chômage en France pour chaque ville\n",
    "   \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#librairies nécessaire\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pandas import DataFrame\n",
    "from multiprocessing import Pool\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping \n",
    "   - ## Villes de France et leurs liens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colonnes de notre data set\n",
    "colonnes=['ville','lien']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation d'un data frame vide pour enrigstrer les ville et les liens \n",
    "tableau=DataFrame(columns=colonnes)\n",
    "# sauvgarder Data set vide en csv\n",
    "tableau.to_csv('C:/Users/lamine/PycharmProjects/dataset/lienVille.csv',index=False)\n",
    "dico={}\n",
    "dico['ville']=''\n",
    "dico['lien']=''\n",
    "# lien pour recuprer les lien de la ville\n",
    "url=\"http://www.journaldunet.com/management/ville/index/villes?page=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ouverture du data set avec with open avec l'encoding utf-8\n",
    "with open('C:/Users/lamine/PycharmProjects/dataset/lienVille.csv','a',encoding = \"utf-8\") as csvfile:\n",
    "    # Ecriture des données recuperer dans un dictionnaire\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=colonnes, lineterminator='\\n')\n",
    "    # Parcourir toutes les page du site\n",
    "    for numeroPage in range(1,701):\n",
    "        print('le numero de page en traitement est : '+ str(numeroPage))\n",
    "        # se connecter au l'url du site \n",
    "        req = requests.get(url+str(numeroPage))\n",
    "        # recuperer le contenu html du la page \n",
    "        contenu = req.content\n",
    "        soup = bs(contenu, 'html.parser')\n",
    "        tousLeslines = soup.findAll('a')\n",
    "        # remplissage du dico\n",
    "        for lien in tousLeslines:\n",
    "            if '/ville-'in lien['href'] and lien['href'][:11]=='/management':\n",
    "                dico['lien']=lien['href']\n",
    "                dico['ville']=lien.text\n",
    "                # ecriture par ligne sur le dictinnaire \n",
    "                writer.writerow(dico)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Informmations sur les villes de france "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colonnes de notre dataframe\n",
    "colonnes = ['ville', 'lien', 'Région', 'Département',\n",
    "                'Etablissement public de coopération intercommunale (EPCI)',\n",
    "                'Code postal (CP)','Code Insee',\n",
    "                'Nom des habitants',\n",
    "                'Population (2017)',\n",
    "                'Population : rang national (2017)',\n",
    "                'Densité de population (2017)',\n",
    "                'Taux de chômage (2016)',\n",
    "                'Pavillon bleu',\n",
    "                \"Ville d'art et d'histoire\",\n",
    "                'Ville fleurie',\n",
    "                'Ville internet',\n",
    "                'Superficie (surface)',\n",
    "                'Altitude min.',\n",
    "                'Altitude max.',\n",
    "                'Latitude',\n",
    "                'Longitude' ]\n",
    "\n",
    "\n",
    "#le fichier des inforation existe ou pas\n",
    "if os.path.isfile('C:/Users/lamine/PycharmProjects/dashboard1/dataset/info.csv'):\n",
    "\n",
    "    tableauInfos=pd.read_csv('C:/Users/lamine/PycharmProjects/dashboard1/dataset/info.csv',error_bad_lines=False, dtype='unicode')\n",
    "    tableauLiens=pd.read_csv('C:/Users/lamine/PycharmProjects/dashboard1/dataset/lienVille.csv')\n",
    "    \n",
    "else:\n",
    "\n",
    "    # creation de notre dataframe info\n",
    "    tableauInfos = DataFrame(columns=colonnes)\n",
    "    tableauInfos.to_csv('C:/Users/lamine/PycharmProjects/dashboard1/dataset/info.csv', index=False)\n",
    "    # recupere la liste des liens \n",
    "    tableauLiens=pd.read_csv('C:/Users/lamine/PycharmProjects/dashboard1/dataset/lienVille.csv')\n",
    "    listeLiens = tableauLiens['lien']\n",
    "# verefication des liens \n",
    "listLiens=[i for i in listeLiens if str(i[:11])=='/management']\n",
    "\n",
    "\n",
    "\n",
    "def parse(lien):\n",
    "    #initialisation d'un dictionnaire\n",
    "    dico = {i: '' for i in colonnes}\n",
    "    colonne=dico.keys()\n",
    "    # url du site (site + lien pour la ville)\n",
    "    req = requests.get(\"http://www.journaldunet.com\" + lien)\n",
    "   \n",
    "    # pose de 2 seconde apres l'exucution des ligne d'avant\n",
    "    time.sleep(4)\n",
    "    if req.status_code==200: # satuts code =200 c'est il s'est conecter à la page ou pas (404 non exist .....)\n",
    "        with open ('C:/Users/lamine/PycharmProjects/dashboard1/dataset/info.csv','a',encoding = \"utf-8\") as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=colonne,lineterminator ='\\n')\n",
    "\n",
    "            contenu=req.content\n",
    "            soup=bs(contenu,'html.parser')\n",
    "            tables = soup.findAll('table', class_='odTable odTableAuto')\n",
    "\n",
    "            dico['lien'] = lien\n",
    "            dico['ville'] = tableauLiens[tableauLiens['lien'] == lien]['ville'].iloc[0]\n",
    "\n",
    "            for i in range(len(tables)):\n",
    "                tousLesTr = tables[i].findAll('tr')\n",
    "                for tr in tousLesTr[1:]:\n",
    "                    cle = tr.findAll('td')[0].text\n",
    "                    valeur = tr.findAll('td')[1].text\n",
    "\n",
    "                    if \"Nom des habitants\" in cle:\n",
    "                        dico[\"Nom des habitants\"] = str(valeur)\n",
    "                    elif \"Taux de chômage\" in cle:\n",
    "                        dico[\"Taux de chômage (2016)\"] = str(valeur)\n",
    "                    else:\n",
    "                        dico[cle] = str(valeur)\n",
    "            time.sleep(1)\n",
    "            writer.writerow(dico)\n",
    "            print(dico)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Multiprocessing avec la fonction Pool\n",
    "    with Pool(15) as p:\n",
    "        p.map(parse, listeLiens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
